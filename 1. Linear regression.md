# 单变量线性回归 
## 1. 房价的训练集 （Training set）
   + notations：
     + m：training examples的数量
     + x：输入变量
     + y：输出变量
     + (x, y)：单个的训练样本，如果表示第i个样本，则在x和y的右上角添加 (i) 来表示
   + 假设函数（Hypothesis）
## 2. 代价函数（cost function）
   + 假设函数：h(x) = θ1 + θ2x（对于给定的θ1和θ2，h(x)是x的函数）
   + 目标是，找到合适的θ1和θ2使得sum( (h(x) - y)^2 )/2m 最小（预测值和实际值差的平方和的平均值最小）
   + 代价函数也叫平方误差函数（squared error function）：J(θ1, θ2) = sum( (h(x) - y)^2 )/2m
   + 平方误差函数对大多数回归问题都是一个合适的选择
## 3. 梯度下降法（gradient descend：最小化代价函数）
   + 代价函数：J(θ1, θ2, θ3,... θn)
      + 给定一些θi（i=1, 2, 3,...n）的初始值（例如，都设为0）；
      + 逐渐改变θi，以使J逐渐减小，直至找到最小值（局部最小值）。
   + 初始值不同，可能会收敛于不同的局部最优值。
   + 算法：
      + repeat until convergence{
          θi := θi - αdJ/dθi  (for i = 1, 2, 3, ...n)
        }
         + := 表示赋值运算符；
         + α 表示学习率（learning rate）：太大，则梯度下降会太慢；太小，则梯度下降会过冲最小值，可能无法收敛
         + dJ/dθi表示J对某一个参数θi的偏导数
      + **同时更新所有的θi**  
         ```python
         t0 = theta0 - aJ0'
         t1 = theta1 - aJ1'
         theta0 = t0
         theta1 = t1
         ```
      + 如果θi已经使得J处于最小值，则梯度下降算法应保持θi使得J仍处于最小值。
      + 对梯度下降算法而言，即便学习率固定，其对θi所进行的步进也会随着J的收敛而越来越小。
## 4. 线性回归的梯度下降法
   + 代价函数：J = sum( (θ1+θ2x - y)^2 )/2m; (x和y取所有训练集)
   + 偏导项： dJ/dθ0 = sum( (h(x) - y)^2 ) / m;   
             dJ/dθ1 = sum( (h(x) - y)^2 ) * x/m;
   + 对于线性回归问题，其代价函数通常呈凸函数类型（convex function）
   + 又称 Batch Gradient Descent
      + 梯度下降的每一个步进都遍历了所有的训练集
   + 正规方程组方法（normal equations methods）可以替代梯度下降法来求解线性回归问题（线性代数知识），但是对于较大的训练集，梯度下降方法更适合。   
# 多变量线性回归
## 1. 多特征量（Multiple features/variables）
   + 假定特征量的数量是n,则每一个训练集的输入都是一个n维的向量（h(x) = θ1x1 + θ2x2 + ... + θnxn）
   + 向量形式：参数向量Θm = [θ1, θ2, ... , θn]， 特征向量xm = [x1, x2, ... , xn]，则h(x) = transpose(Θm)xm
## 2. 多元梯度下降法
   +  ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95.PNG)
   + 上图左边是单变量线性回归，右边是多变量线性回归。
   + **特征缩放（feature scaling）**
      + 使特征量的取值范围在相似的量级上，以使得梯度下降算法更快地收敛。
      + 这一技巧是指，假设h(x) = θ1x1 + θ2x2，如果x1和x2的取值范围差异比较大的话，应该想办法对较大者进行缩放，如将其除以一个因子。
      + 理想情况下，希望将特征量缩放至近似为[-3, 3]的范围内。-1和1的确切值不是必须的，如[0, 3], [-2, 0.5]也是可以的。
      + 均值归一化（mean normalization）
         + 把xi变换为xi-mean(xi)，然后再进行缩放。其中mean(xi)是第i个特征量（xi）的均值。
         + 一般地，缩放因子可以设置为特征量的取值范围（最大值减去最小值）。
   + **学习率（learning rate）**
      + 判断梯度下降算法在正确运行：
         + 画出J(θ)随迭代次数变化的曲线，每一次迭代后，J(θ)都应该在下降。
         + 如果发现J(θ)在上升，则一般需要减小学习率。
         + 如果发现J(θ)出现震荡，则一般也需要减小学习率。
      + 如果一次迭代所引起的J(θ)的变化小于某一个阈值，则可以自动认为已经收敛。但是阈值的选择通常比较困难，如1/1000或1/100000。
      + 选择学习率的时候，可以尝试0.001， 0.003， 0.01， 0.03， 0.1， 0.3， 1， ...等，以3的倍数递增。
## 3. 特征和多项式回归（features and polynomial regression）
   + 对于类似h(x) = θ0 + θ1x + θ2x^2 + θ3x^3的假设函数，可以用h(x) = θ0 + θ1x1 + θ2x2 + θ3x3，即特征重建的方式来使其可以用线性回归的方式处理。其中x1 = x, x2 = x^2, x3 = x^3。
## 4. 正规方程（normal equation）
   + 正规方程是一种解析地求解θ使得J(θ)最小的方式。
   + 假设输入训练集有n个features，共m个样本，则θ = (X^T*X)^-1*X^T*Y，其中X和Y分别是输入和输出参数矩阵，X^T是X的转置，X^-1是X的逆。X是m*(n+1)的矩阵，Y是m*1的矩阵。
   + ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png)
   + 正规方程无需特征缩放。
   + **由于(X^T*X)是一个n*n的矩阵，如果n很大，则该矩阵求逆会很慢(O(n^3))，此时用梯度下降法会更好。n小于10000用正规方程是OK的。**
## 5. 正规方程及不可逆性（normal equation and non-invertibility）
   + 不可逆问题指的是(X^T * X)可能是不可逆矩阵（奇异（singular）或退化矩阵（degenerate））。
   + 不可逆通常有如下两种情况：
      + 包含了多余的features（features线性相关）；-> 如x1 = 3.14 * x2
      + 太多的features（如m小于或等于n）；=> 删除一些features，或者正则化（regularization）
   + 事实上，即使出现不可逆问题，数值方法也可求出最优的θ（伪逆函数手段），如Octave中的pinv()函数总能计算(X^T * X)^-1
   
      
