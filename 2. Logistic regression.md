# Logistic回归算法（分类问题：Classification problem）
## 1. 分类问题
  + 邮件：垃圾邮件（Yes/No）
  + 输出为离散的取值：
      + 如0（negative class）或1（positive class）=> 二元分类问题
      + y = {0, 1, 2, 3...} => 多元分类问题
  + 逻辑回归：0 ≤ h(x) ≤ 1
## 2. 假设陈述（hypothesis representation）
  + 假设函数：h(x) = g(θ^T * x)，其中g(z) = 1/(1+exp(-z))是sigmoid函数或者叫logistic函数。
  + h(x)的输出是指：对于一个新的输入x，其预估的y=1的概率。（ h(x) = P (y=1|x; θ) ）
## 3. 决策边界（Decision boundary）
  + sigmoid函数即g(z) = 1/(1+exp(-z))在z ≥ 0时≥0.5（positive），在z < 0时<0.5（negative）
    ```
    θ^T * x ≥ 0 => 预测y=1
    θ^T * x < 0 => 预测y=1
    ```
  ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%86%B3%E7%AD%96%E7%95%8C%E9%99%90.png)
  + 决策边界由假设函数及参数θ决定，训练集只是用来拟合θ，而与决策边界无关。
## 4. 代价函数（cost function）
  + 如果类似线性回归方式，定义代价函数为：J(θ) = 1/m * sum(Cost(h(x), y))， 其中：
    + Cost(h(x), y) = 1/2*(h(x) - y)^2)
    + h(x) = 1/(1+exp(-θ^T * x))
    + 该代价函数是非凸函数（non-convex），存在不止一个局部最优解。
  + **逻辑回归代价函数**
    ```python
    if y = 1:
      Cost(h(x), y) = -log(h(x))      # 注意：0 < h(x) < 1
    elif y = 0:
      Cost(h(x), y) = -log(1-h(x))
    ```
