# 正则化（Regularization）
## 过度拟合问题（overfitting）
  + 欠拟合（underfitting）-> 高偏差（high bias）：未能很好的拟合数据集；
  + 过度拟合（overfitting）-> 高方差（high variance）：变量(features)过多，假设函数虽然能够很好地拟合训练集，但无法泛化（generalize）到新的样本（无法预测新样本的结果）；
  + 过拟合问题的解决办法：
    + 绘制假设模型曲线 => 选择合适的多项式阶次（保留部分变量，舍弃部分变量） => 变量很多的时候，并不合适，而且绘图很困难。
    + **减少变量的数量：**
      + 人工选择保留的变量；
      + 模型选择算法（model selection algorithm）=> 自动选择需要保留的变量；
      + 舍弃变量也就意味着丢弃了关于问题的一些信息；
    + **正则化：**
      + 保留所有变量，减小参数θ的量级/大小；
      + 对于很多变量，每个变量对输出有一点贡献的情况很适用；
## 代价函数
  + 参数值θ0, θ1, ...θn较小（参数惩罚，penalizing）
    + 更简单的假设函数；
    + 更不易过拟合；
  + 如何选择需要惩罚的参数？
    + 惩罚所有的参数 => 新的代价函数：J(θ) = 1/2m * ( sum( (h(x) - y)^2) + sum(lambda * θ^2) )，其中：
      + (h(x) - y)^2求和针对m个训练集，(lambda * θ^2)求和针对n个参数。
      + 代价函数的第一项是为了最好地拟合（目的1），第二项是为了使得参数尽可能小（目的2）；
      + lambda是正则化参数，是上述两个目的的折中；
      + 如果lambda很大 => 欠拟合  
  ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%82%E6%95%B0%E6%83%A9%E7%BD%9A.png)
## 正则化线性回归
  + 代价函数：J(θ) = 1/2m * ( sum( (h(x) - y)^2) + sum(lambda * θ^2) )
  + 梯度下降算法：
    ```python
    Repeat{
      θ0 := θ0 - α/m * sum( (h(x) - y) * x0 )
      θj := θj - α/m * ( sum( (h(x) - y) * xj ) + lambda * θj)   # 等效为：θj := θj(1 - α * lambda/m) - α/m * sum( (h(x) - y) * xj )
      }
    ```
    + 注意```(1 - α * lambda / m)```这一项，这一项仅比1略小：学习率比较小，m比较大
  + 正规方程算法
    + X = [x1, x2, ... xm], y = [y1, y2, ... ym]
    + Θ = (X^T * X + lambda * L)^-1 * x^T * y，其中
      + L是(n+1)×(n+1)的矩阵，该矩阵的对角线除L11 = 0外，其余元素均为1；除对角线外，其余元素均为0；（n是features的数量）
      + 如果lambda>0, 则 (X^T * X + lambda * L)一定是可逆的
## 正则化逻辑回归     
  + 代价函数：J(θ) = -1/m * sum(y * log(h(x)) + (1-y) * log(1-h(x))) + sum(lambda * θ^2)/2m
  + 梯度下降算法：
    ```python
    Repeat{
      θ0 := θ0 - α/m * sum( (h(x) - y) * x0 )
      θj := θj - α/m * (sum( (h(x) - y) * xj ) + lambda * θj )    # 等效为：θj := θj(1 - α * lambda/m) - α/m * sum( (h(x) - y) * xj )
      }
    ```
  + fminunc函数（无约束条件下的最小值函数）
