# 神经网络的表示（Representation）
## 非线性假设（nonlinear hypotheses）
  + 非线性分类问题
    + g(θ0 + θ1x1 + θ2x2 + θ3x1x2 + θ4x1^2x2 + ...)
    + 当features很多时，即[x1, x2, x3, ...]的元素数很大时，上述的假设函数显然是不合适的
## 神经网络模型
  + 神经网络模型：
    + 构成：输入节点(x0, x1, x2...)，参数（θ0, θ1, θ2, ...），激活项（a21, a22, a23..., a31, a32,...），激活函数（activation function：h(x)）
      + 参数（parameters）有时也称为权重（weight）
      + x0也称为偏置单元（bias unit），常设置为1
      + 较常用的激活函数是sigmoid激活函数（ h(x) = 1/(1+exp(-Θ^T * X)) ）
      + 激活项a21表示第2层的第1个激活单元  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.png)  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2.png)
  + 前向传播：矢量化实施（Forward propagation: Vectorized implementation）
    + 对于一个三层的神经网络，假设：
      + 输入节点为 X = [x0, x1, x2, x3]，也可以将其定义为a1
      + 参数为 Θ1 = [ [θ110, θ111, θ112, θ113]; [θ120, θ121, θ122, θ123]; [θ130, θ131, θ132, θ133] ]
        + θijk表示第i层第j个激活项(aij)的第k个参数，有aij = g(θij0 * x0 + θij1 * x1 + θij2 * x2 + θij3 * x3)
      + 定义Z2 = [z21, z22, z32]
        + 其中，z2i = [θ2i0 * x0 + θ2i1 * x1 + θ2i2 * x2 + θ2i3 * x3] = Θ1i * X
      + 激活项a2 = g(Z2)
      + 输出激活函数h(x) = g(θ20 * a20 + θ21 * a21 + θ22 * a22 + θ23 * a23) 
      + 定义Z3 = Θ2 * a2，则h(x) = g(Z3)  
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD.png)
    + 神经网络很像逻辑回归，不同仅在于其特征量(features)不是原始的features，而是隐藏层计算所得的激活项。
## 神经网络例子I
  
