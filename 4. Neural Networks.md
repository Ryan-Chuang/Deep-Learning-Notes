# 神经网络的表示（Representation）
## 非线性假设（nonlinear hypotheses）
  + 非线性分类问题
    + g(θ0 + θ1x1 + θ2x2 + θ3x1x2 + θ4x1^2x2 + ...)
    + 当features很多时，即[x1, x2, x3, ...]的元素数很大时，上述的假设函数显然是不合适的
## 神经网络模型
  + 神经网络模型：
    + 构成：输入节点(x0, x1, x2...)，参数（θ0, θ1, θ2, ...），激活项（a21, a22, a23..., a31, a32,...），激活函数（activation function：h(x)）
      + 参数（parameters）有时也称为权重（weight）
      + x0也称为偏置单元（bias unit），常设置为1
      + 较常用的激活函数是sigmoid激活函数（ h(x) = 1/(1+exp(-Θ^T * X)) ）
      + 激活项a21表示第2层的第1个激活单元  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.png)  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C2.png)
  + 前向传播：矢量化实施（Forward propagation: Vectorized implementation）
    + 对于一个三层的神经网络，假设：
      + 输入节点为 X = [x0, x1, x2, x3]，也可以将其定义为a1
      + 参数为 Θ1 = [ [θ110, θ111, θ112, θ113]; [θ120, θ121, θ122, θ123]; [θ130, θ131, θ132, θ133] ]
        + θijk表示第i层第j个激活项(aij)的第k个参数，有aij = g(θij0 * x0 + θij1 * x1 + θij2 * x2 + θij3 * x3)
      + 定义Z2 = [z21, z22, z32]
        + 其中，z2i = [θ2i0 * x0 + θ2i1 * x1 + θ2i2 * x2 + θ2i3 * x3] = Θ1i * X
      + 激活项a2 = g(Z2)
      + 输出激活函数h(x) = g(θ20 * a20 + θ21 * a21 + θ22 * a22 + θ23 * a23) 
      + 定义Z3 = Θ2 * a2，则h(x) = g(Z3)  
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD.png)
    + 神经网络很像逻辑回归，不同仅在于其特征量(features)不是原始的features，而是隐藏层计算所得的激活项。
## 神经网络例子
  + 逻辑与神经网络计算例子：
  ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E9%80%BB%E8%BE%91%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)
  + 逻辑非异或（XNOR）神经网络例子：
  ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E9%80%BB%E8%BE%91XNOR%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)
## 多类别分类
  + 输出是多值情况(类似one-vs-all method)：
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98.png)
## 神经网络反向传播算法
  + **代价函数**  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png)  
    + 其中：
      + K表示输出单元的数量（对于多类别分类问题，K即classes的数量）
      + L表示神经网络的层的总数
      + sl表示第l层的神经元的数量
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)
  + **反向传播算法**  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%951.png)  
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%952.png)  
  + **反向传播算法直觉（intuition）**
    + 前向传播实施（假定共四层神经网络）
      + 给定输入样本x，初始化参数θ1，计算z2，然后应用sigmoid函数，计算a2；
      + 以a2为输入，初始化参数θ2，计算z3，然后应用sigmoid函数，计算a3；
      + 以a3位输入，初始化参数θ3，计算z4，然后应用sigmoid函数，计算a4，即输出；
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%AE%9E%E6%96%BD.png)
    + 反向传播实施（继承上述的前向传播实施）
      + 假定已经根据前向传播实施算法计算获得a4，则：
      + 根据a4和输出y，计算δ4；（δ4表示第4层的代价函数的误差）
      + 根据参数θ3和δ4，计算δ3；
      + 根据参数θ2和δ3，计算δ2；  
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88.png)  
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82.png)  
  + **使用注意：展开参数（unrolling parameters from matrices into vectors）**  
    + 使用高级优化步骤（advanced optimization routines）中需要；
    + 展开参数指的是，将所有的Θ（每一层的参数，矩阵，维度为前一层输入节点个数 X 当前层的神经元数量）以及计算所得的梯度项D（每一层的代价函数J(θ)对每一层的每一个参数的偏导数，矩阵，维度与Θ一致），展开成一个一行多列的向量。
    + 参数展开后，即可应用在Octave等中的fminunc函数中；
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95Octave.png)
    ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E5%8F%82%E6%95%B0%E5%B1%95%E5%BC%80.png)
    + 神经网络学习算法：
      + 初始化每一层的参数Θ；
      + 展开参数Θ，获得initialTheta，将该参数传给fminunc(@costFunction, initialTheta, options)；
      + 定义costFunction：
        + function [jval, gradientVec] = costFunction(thetaVec)
          + 根据thetaVec, 获得每一层的Θ（reshape，将向量抽出，整形为每一层的矩阵）；
          + 使用前向传播、反向传播方法计算每一层的每个参数的梯度D和代价函数J(θ)；
          + 将梯度D展开，获得gradientVec；
      ![image](https://github.com/Ryan-Chuang/DL_IMGS/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95.png)
  + 
